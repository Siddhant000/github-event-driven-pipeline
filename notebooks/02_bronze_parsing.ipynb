from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName("GitHub").getOrCreate()

df_raw = spark.read.option('columnNameOfCorruptRecord','_corrupt_record').option("multiline",'true').option("mergeSchema", "true").format("json").load("/Volumes/workspace/default/raw_github_events/")
# using multiline, as there are records which have nested json data.
# using mergeSchema for Schema evolution

df_main = df_delta.select(col('id').alias('event_id'),
                          col('type').alias('event_type'),
                          to_timestamp(col('created_at')).alias('created_at'),
                          struct('*').alias('raw_event')
                          )
df_main = df_main.withColumn('ingestion_time',current_timestamp())
df_main = df_main.withColumn('source',lit('github_public_api'))

df_main.write \
  .format("delta") \
  .mode("append") \
  .save("/Volumes/workspace/default/delta/delta1/")
